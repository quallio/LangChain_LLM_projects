{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBwOigkft0bA"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There exists two Hugging Face LLM wrappers, one for a local pipeline and one for a model hosted on Hugging Face Hub.\n",
        "Note that these wrappers only work for models that support the following tasks: text2text-generation, text-generation\n",
        "\n",
        "To use the local pipeline wrapper:"
      ],
      "metadata": {
        "id": "gCizPChPx3Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.llms import HuggingFacePipeline"
      ],
      "metadata": {
        "id": "_a9Y2xIEubcY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use a the wrapper for a model hosted on Hugging Face Hub:"
      ],
      "metadata": {
        "id": "YaMvoy2ix9JM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub"
      ],
      "metadata": {
        "id": "QzY17V9sx_ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJZFp1k1w4T-",
        "outputId": "dbcf5ae9-238e-4648-f792-fadc73715180"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "id": "XROflkLPxart"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare examples from LangChain documentation\n"
      ],
      "metadata": {
        "id": "pKF3ESOmxu48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n",
        "from langchain import PromptTemplate, LLMChain"
      ],
      "metadata": {
        "id": "f3J-d22hyGPD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "EYeDY3ZvyVAt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are some examples of models you can access through the Hugging Face Hub integration"
      ],
      "metadata": {
        "id": "gBPFq7jcynRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flan, by Google"
      ],
      "metadata": {
        "id": "-SIIQ-X0yrL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"google/flan-t5-xxl\""
      ],
      "metadata": {
        "id": "nH_q9NEEyvWj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look here for other options:\n",
        "https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads"
      ],
      "metadata": {
        "id": "rzsLwgU9y8eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
        ")\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "u2T7hIqDzDTT"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrCe9AP6zM9k",
        "outputId": "d8c0380a-b140-4224-a619-423cd7021767"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The FIFA World Cup in 1994 was won by France. The answer: France.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dolly, by Databricks"
      ],
      "metadata": {
        "id": "m8tUjcSY0MSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"databricks/dolly-v2-3b\""
      ],
      "metadata": {
        "id": "eH4SVEqU0SAE"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
        ")\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "MqCijyYn0XoD"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePdbL6zy0epz",
        "outputId": "5552274e-99ed-4ea4-bfd3-b0998c4c5953"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.\n",
            "\n",
            "\n",
            "Question: Who\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Camel, by Writer"
      ],
      "metadata": {
        "id": "fy-v-p4b0p4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"Writer/camel-5b-hf\""
      ],
      "metadata": {
        "id": "hAFzWVdC0twE"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
        ")\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "ky_4nHwa0wbj"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "b585RYsS00Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Falcon, by TechnologyInnovation Institute"
      ],
      "metadata": {
        "id": "Nrlm-bir1HZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"tiiuae/falcon-40b\""
      ],
      "metadata": {
        "id": "f2YZj8yy1MQr"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
        ")\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "6JggGIsJ1NU6"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "s0k8UY0X2Ndk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}